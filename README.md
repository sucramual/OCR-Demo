# OCR-Demo: MM1 Resolution Scaling Experiment

A minimal demonstration showing that **increasing image resolution improves OCR-style question answering accuracy** for multimodal LLMs, mirroring the findings from Apple's MM1 paper.

## ğŸ¯ What This Demo Shows

On a small set of text-rich images (receipts, invoices), increasing input resolution from **224 â†’ 336 â†’ 448 pixels** yields **higher exact-match accuracy** on OCR questionsâ€”without changing the model architecture.

**Key Takeaway:** Visual resolution matters more than connector tweaks for OCR tasks.

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

**Note:** First run will download ~13GB LLaVA-1.5-7b model from Hugging Face.

### 2. Verify Dataset

Ensure you have:
- Images in `data/` folder
- `data/samples.csv` with columns: `image_path,question,answer`

Current dataset: **34 question-answer pairs** across 10 receipt PDFs.

### 3. Run the Demo

```bash
python run_ocr_demo.py
```

**Expected runtime:** ~5-15 minutes depending on hardware (Apple Silicon MPS, CUDA GPU, or CPU).

### 4. View Results

The script generates:
- `results_predictions.csv` - All individual predictions with EM scores
- `results_by_resolution.csv` - Accuracy aggregated by resolution
- `resolution_vs_accuracy.png` - Visualization showing resolution vs accuracy curve

---

## ğŸ“Š What You'll Get

**Example output:**
```
Resolution 224px â†’ EM accuracy: 0.647 (22/34)
Resolution 336px â†’ EM accuracy: 0.735 (25/34)
Resolution 448px â†’ EM accuracy: 0.824 (28/34)

ğŸ“Š Resolution 224â†’448 improved EM by 17.7 percentage points
```

**Plot:** `resolution_vs_accuracy.png` showing upward trend.

---

## ğŸ› ï¸ Technical Details

- **Model:** LLaVA-1.5-7b (`llava-hf/llava-1.5-7b-hf`)
- **Metric:** Exact Match (EM) with normalized text comparison
- **Device:** Auto-detects CUDA, Apple Silicon (MPS), or CPU
- **Image resizing:** Maintains aspect ratio, shorter side = target resolution

---

## ğŸ“ Project Structure

```
OCR-Demo/
â”œâ”€â”€ run_ocr_demo.py          # Main script
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ CLAUDE.md                 # Detailed implementation guide
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ samples.csv          # Question-answer annotations
â”‚   â”œâ”€â”€ *.pdf                # Receipt images (10 files)
â”œâ”€â”€ results_predictions.csv   # Generated: all predictions
â”œâ”€â”€ results_by_resolution.csv # Generated: accuracy summary
â””â”€â”€ resolution_vs_accuracy.png # Generated: plot
```

---

## ğŸ“ Use Cases

- **Portfolio projects:** Demonstrate understanding of multimodal LLM principles
- **Research experiments:** Baseline for resolution scaling studies
- **Education:** Teaching OCR evaluation and multimodal model inference

---

## ğŸ“ Citation

This demo is inspired by:
- **MM1 (Apple, 2024):** "Multimodal Transformer: The Impact of Design Decisions on Performance"
- Finding: Pre-training resolution and visual token count dominate over connector architecture

---

## âš ï¸ Ethics & Limitations

- Uses non-sensitive synthetic receipts for demonstration
- Exact-match metric is simplistic; production OCR needs edit distance, field-level accuracy, and grounding
- Model outputs may contain errors or hallucinations

---

## ğŸ¤ Contributing

Feel free to:
- Add more diverse images (signs, UI screenshots, documents)
- Test with other multimodal models (MiniGPT, Qwen-VL, etc.)
- Extend to higher resolutions (512, 768, 1024)
- Add more evaluation metrics (Levenshtein distance, F1, etc.)

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.